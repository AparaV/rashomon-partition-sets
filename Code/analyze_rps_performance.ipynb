{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d976c5",
   "metadata": {},
   "source": [
    "# RPS Performance Analysis\n",
    "\n",
    "This notebook analyzes the performance results from the Rashomon Partition Set (RPS) algorithm simulation study. \n",
    "\n",
    "The simulation varies:\n",
    "- **M**: Number of features (3, 4, 5)\n",
    "- **R**: Factor levels per feature (3, 4, 5) \n",
    "- **H**: Maximum number of pools (multipliers: 1.0, 1.5, 2.0)\n",
    "- **Œµ (epsilon)**: Rashomon threshold (0.5, 1.0, 2.0, 4.0)\n",
    "\n",
    "For each parameter combination, we analyze:\n",
    "- **Runtime performance** of the RPS algorithm\n",
    "- **Accuracy** in terms of pool mean errors\n",
    "- **Coverage** comparing RPS partitions found vs. total possible partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb712759",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1958d9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62afc3d6",
   "metadata": {},
   "source": [
    "## 2. Load the Results Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d79d614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the simulation results\n",
    "try:\n",
    "    df = pd.read_csv(\"rps_performance_results.csv\")\n",
    "    print(f\"‚úÖ Successfully loaded {len(df)} simulation results\")\n",
    "    print(f\"üìä Data shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Results file not found. Please run the simulation first:\")\n",
    "    print(\"   python rps_performance_simulation.py\")\n",
    "    df = None\n",
    "\n",
    "if df is not None:\n",
    "    # Add derived columns for analysis\n",
    "    df['complexity'] = df['M'] * df['R_val']\n",
    "    df['H_relative'] = df['H'] / (df['M'] * df['R_val'])  # H relative to problem size\n",
    "    df['coverage_ratio'] = df['num_rps_partitions'] / df['num_total_partitions']\n",
    "    df['log_rps_time'] = np.log10(df['rps_time'] + 1e-6)  # Log transform for better visualization\n",
    "    \n",
    "    print(\"üìà Added derived columns: complexity, H_relative, coverage_ratio, log_rps_time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b080773",
   "metadata": {},
   "source": [
    "## 3. Explore Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4b8e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"=== DATA OVERVIEW ===\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "    print(\"\\n=== FIRST FEW ROWS ===\")\n",
    "    display(df.head())\n",
    "    \n",
    "    print(\"\\n=== DATA TYPES ===\")\n",
    "    display(df.dtypes)\n",
    "    \n",
    "    print(\"\\n=== PARAMETER RANGES ===\")\n",
    "    param_summary = df[['M', 'R_val', 'H', 'epsilon']].describe()\n",
    "    display(param_summary)\n",
    "    \n",
    "    print(\"\\n=== UNIQUE PARAMETER COMBINATIONS ===\")\n",
    "    unique_combinations = df.groupby(['M', 'R_val', 'H', 'epsilon']).size().reset_index(name='count')\n",
    "    print(f\"Total parameter combinations: {len(unique_combinations)}\")\n",
    "    print(f\"Simulations per combination: {unique_combinations['count'].describe()}\")\n",
    "    \n",
    "    print(\"\\n=== MISSING VALUES ===\")\n",
    "    missing_summary = df.isnull().sum()\n",
    "    if missing_summary.sum() > 0:\n",
    "        display(missing_summary[missing_summary > 0])\n",
    "    else:\n",
    "        print(\"‚úÖ No missing values found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4874f3d8",
   "metadata": {},
   "source": [
    "## 4. Performance Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c3627a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"=== RUNTIME PERFORMANCE ANALYSIS ===\")\n",
    "    \n",
    "    # Overall timing statistics\n",
    "    timing_stats = df['rps_time'].describe()\n",
    "    print(\"üìä Overall Runtime Statistics:\")\n",
    "    display(timing_stats)\n",
    "    \n",
    "    # Timing by complexity\n",
    "    print(\"\\nüìà Runtime by Problem Complexity (M √ó R):\")\n",
    "    timing_by_complexity = df.groupby('complexity')['rps_time'].agg(['count', 'mean', 'std', 'min', 'max'])\n",
    "    display(timing_by_complexity)\n",
    "    \n",
    "    # Timing by individual parameters\n",
    "    print(\"\\nüîç Runtime by M (Number of Features):\")\n",
    "    timing_by_M = df.groupby('M')['rps_time'].agg(['mean', 'std'])\n",
    "    display(timing_by_M)\n",
    "    \n",
    "    print(\"\\nüîç Runtime by R (Factor Levels):\")\n",
    "    timing_by_R = df.groupby('R_val')['rps_time'].agg(['mean', 'std'])\n",
    "    display(timing_by_R)\n",
    "    \n",
    "    print(\"\\nüîç Runtime by H (Max Pools):\")\n",
    "    timing_by_H = df.groupby('H')['rps_time'].agg(['mean', 'std'])\n",
    "    display(timing_by_H)\n",
    "    \n",
    "    print(\"\\nüîç Runtime by Epsilon (Rashomon Threshold):\")\n",
    "    timing_by_epsilon = df.groupby('epsilon')['rps_time'].agg(['mean', 'std'])\n",
    "    display(timing_by_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c517f152",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"=== ACCURACY ANALYSIS ===\")\n",
    "    \n",
    "    # Pool mean error statistics\n",
    "    error_stats = df['mean_pool_error'].describe()\n",
    "    print(\"üìä Pool Mean Error Statistics:\")\n",
    "    display(error_stats)\n",
    "    \n",
    "    # Error by epsilon (Rashomon threshold)\n",
    "    print(\"\\nüéØ Error by Epsilon (Rashomon Threshold):\")\n",
    "    error_by_epsilon = df.groupby('epsilon')['mean_pool_error'].agg(['count', 'mean', 'std', 'min', 'max'])\n",
    "    display(error_by_epsilon)\n",
    "    \n",
    "    # Error by complexity\n",
    "    print(\"\\nüìà Error by Problem Complexity:\")\n",
    "    error_by_complexity = df.groupby('complexity')['mean_pool_error'].agg(['mean', 'std'])\n",
    "    display(error_by_complexity)\n",
    "    \n",
    "    # Coverage analysis\n",
    "    print(\"\\n=== COVERAGE ANALYSIS ===\")\n",
    "    coverage_stats = df['coverage_ratio'].describe()\n",
    "    print(\"üìä Coverage Ratio Statistics (RPS found / Total possible):\")\n",
    "    display(coverage_stats)\n",
    "    \n",
    "    print(\"\\nüéØ Coverage by Epsilon:\")\n",
    "    coverage_by_epsilon = df.groupby('epsilon')['coverage_ratio'].agg(['mean', 'std'])\n",
    "    display(coverage_by_epsilon)\n",
    "    \n",
    "    # Partition count analysis\n",
    "    print(\"\\n=== PARTITION COUNT ANALYSIS ===\")\n",
    "    print(\"üìä RPS Partitions Found:\")\n",
    "    rps_partition_stats = df['num_rps_partitions'].describe()\n",
    "    display(rps_partition_stats)\n",
    "    \n",
    "    print(\"\\nüìä Total Possible Partitions:\")\n",
    "    total_partition_stats = df['num_total_partitions'].describe()\n",
    "    display(total_partition_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e630672",
   "metadata": {},
   "source": [
    "## 5. Visualize Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d6dd8c",
   "metadata": {},
   "source": [
    "### 5.1 Runtime Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9734f225",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Create a 2x2 subplot for runtime analysis\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('RPS Algorithm Runtime Performance Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Runtime vs Complexity\n",
    "    ax1 = axes[0, 0]\n",
    "    scatter = ax1.scatter(df['complexity'], df['rps_time'], \n",
    "                         c=df['epsilon'], cmap='viridis', alpha=0.6, s=50)\n",
    "    ax1.set_xlabel('Problem Complexity (M √ó R)')\n",
    "    ax1.set_ylabel('Runtime (seconds)')\n",
    "    ax1.set_title('Runtime vs Problem Complexity')\n",
    "    ax1.set_yscale('log')\n",
    "    plt.colorbar(scatter, ax=ax1, label='Epsilon')\n",
    "    \n",
    "    # 2. Runtime distribution by M\n",
    "    ax2 = axes[0, 1]\n",
    "    df.boxplot(column='rps_time', by='M', ax=ax2)\n",
    "    ax2.set_xlabel('Number of Features (M)')\n",
    "    ax2.set_ylabel('Runtime (seconds)')\n",
    "    ax2.set_title('Runtime Distribution by M')\n",
    "    ax2.set_yscale('log')\n",
    "    \n",
    "    # 3. Runtime vs H (max pools)\n",
    "    ax3 = axes[1, 0]\n",
    "    sns.scatterplot(data=df, x='H', y='rps_time', hue='epsilon', ax=ax3)\n",
    "    ax3.set_xlabel('Maximum Pools (H)')\n",
    "    ax3.set_ylabel('Runtime (seconds)')\n",
    "    ax3.set_title('Runtime vs Maximum Pools')\n",
    "    ax3.set_yscale('log')\n",
    "    \n",
    "    # 4. Runtime vs Epsilon\n",
    "    ax4 = axes[1, 1]\n",
    "    sns.boxplot(data=df, x='epsilon', y='rps_time', ax=ax4)\n",
    "    ax4.set_xlabel('Epsilon (Rashomon Threshold)')\n",
    "    ax4.set_ylabel('Runtime (seconds)')\n",
    "    ax4.set_title('Runtime vs Epsilon')\n",
    "    ax4.set_yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dc8b84",
   "metadata": {},
   "source": [
    "### 5.2 Accuracy and Coverage Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c3e859",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Create a 2x2 subplot for accuracy and coverage analysis\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('RPS Algorithm Accuracy and Coverage Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Pool Mean Error vs Epsilon\n",
    "    ax1 = axes[0, 0]\n",
    "    sns.boxplot(data=df, x='epsilon', y='mean_pool_error', ax=ax1)\n",
    "    ax1.set_xlabel('Epsilon (Rashomon Threshold)')\n",
    "    ax1.set_ylabel('Mean Pool Error')\n",
    "    ax1.set_title('Pool Mean Error vs Epsilon')\n",
    "    \n",
    "    # 2. Coverage Ratio vs Epsilon\n",
    "    ax2 = axes[0, 1]\n",
    "    sns.boxplot(data=df, x='epsilon', y='coverage_ratio', ax=ax2)\n",
    "    ax2.set_xlabel('Epsilon (Rashomon Threshold)')\n",
    "    ax2.set_ylabel('Coverage Ratio (RPS/Total)')\n",
    "    ax2.set_title('Coverage Ratio vs Epsilon')\n",
    "    \n",
    "    # 3. RPS Partitions Found vs Total Possible\n",
    "    ax3 = axes[1, 0]\n",
    "    max_val = max(df['num_total_partitions'].max(), df['num_rps_partitions'].max())\n",
    "    ax3.scatter(df['num_total_partitions'], df['num_rps_partitions'], \n",
    "               c=df['epsilon'], cmap='viridis', alpha=0.6, s=50)\n",
    "    ax3.plot([0, max_val], [0, max_val], 'k--', alpha=0.5, label='Perfect Coverage')\n",
    "    ax3.set_xlabel('Total Possible Partitions')\n",
    "    ax3.set_ylabel('RPS Partitions Found')\n",
    "    ax3.set_title('RPS Coverage vs Total Partitions')\n",
    "    ax3.legend()\n",
    "    \n",
    "    # 4. Error vs Coverage Trade-off\n",
    "    ax4 = axes[1, 1]\n",
    "    scatter = ax4.scatter(df['coverage_ratio'], df['mean_pool_error'], \n",
    "                         c=df['epsilon'], cmap='viridis', alpha=0.6, s=50)\n",
    "    ax4.set_xlabel('Coverage Ratio')\n",
    "    ax4.set_ylabel('Mean Pool Error')\n",
    "    ax4.set_title('Error vs Coverage Trade-off')\n",
    "    plt.colorbar(scatter, ax=ax4, label='Epsilon')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616709fa",
   "metadata": {},
   "source": [
    "### 5.3 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bffb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Select numerical columns for correlation analysis\n",
    "    corr_columns = ['M', 'R_val', 'H', 'epsilon', 'complexity', 'rps_time', \n",
    "                    'mean_pool_error', 'coverage_ratio', 'num_rps_partitions', 'num_total_partitions']\n",
    "    \n",
    "    # Compute correlation matrix\n",
    "    corr_matrix = df[corr_columns].corr()\n",
    "    \n",
    "    # Create correlation heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='RdBu_r', center=0,\n",
    "                square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "    plt.title('Correlation Matrix of RPS Performance Metrics', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print strongest correlations\n",
    "    print(\"=== STRONGEST CORRELATIONS ===\")\n",
    "    # Get upper triangle of correlation matrix\n",
    "    corr_pairs = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            corr_pairs.append((\n",
    "                corr_matrix.columns[i], \n",
    "                corr_matrix.columns[j], \n",
    "                corr_matrix.iloc[i, j]\n",
    "            ))\n",
    "    \n",
    "    # Sort by absolute correlation value\n",
    "    corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "    \n",
    "    print(\"Top 10 strongest correlations:\")\n",
    "    for i, (var1, var2, corr) in enumerate(corr_pairs[:10]):\n",
    "        print(f\"{i+1:2d}. {var1} ‚Üî {var2}: {corr:+.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfe2318",
   "metadata": {},
   "source": [
    "## 6. Summary and Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587eb5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"=== RPS ALGORITHM PERFORMANCE SUMMARY ===\")\n",
    "    \n",
    "    # Overall performance metrics\n",
    "    total_sims = len(df)\n",
    "    unique_param_combos = len(df.groupby(['M', 'R_val', 'H', 'epsilon']))\n",
    "    \n",
    "    print(f\"üìä Total Simulations: {total_sims:,}\")\n",
    "    print(f\"üéõÔ∏è  Parameter Combinations: {unique_param_combos}\")\n",
    "    print(f\"‚è±Ô∏è  Runtime Range: {df['rps_time'].min():.4f}s - {df['rps_time'].max():.2f}s\")\n",
    "    print(f\"üéØ Error Range: {df['mean_pool_error'].min():.4f} - {df['mean_pool_error'].max():.4f}\")\n",
    "    print(f\"üìà Coverage Range: {df['coverage_ratio'].min():.1%} - {df['coverage_ratio'].max():.1%}\")\n",
    "    \n",
    "    print(\"\\n=== KEY FINDINGS ===\")\n",
    "    \n",
    "    # Runtime scaling\n",
    "    runtime_by_complexity = df.groupby('complexity')['rps_time'].mean().sort_index()\n",
    "    print(f\"üöÄ Runtime Scaling: {runtime_by_complexity.iloc[0]:.4f}s (M√óR={runtime_by_complexity.index[0]}) ‚Üí {runtime_by_complexity.iloc[-1]:.2f}s (M√óR={runtime_by_complexity.index[-1]})\")\n",
    "    \n",
    "    # Best epsilon for accuracy\n",
    "    error_by_epsilon = df.groupby('epsilon')['mean_pool_error'].mean()\n",
    "    best_epsilon = error_by_epsilon.idxmin()\n",
    "    best_error = error_by_epsilon.min()\n",
    "    print(f\"üéØ Best Accuracy: Œµ = {best_epsilon} (error = {best_error:.4f})\")\n",
    "    \n",
    "    # Best epsilon for coverage  \n",
    "    coverage_by_epsilon = df.groupby('epsilon')['coverage_ratio'].mean()\n",
    "    best_coverage_epsilon = coverage_by_epsilon.idxmax()\n",
    "    best_coverage = coverage_by_epsilon.max()\n",
    "    print(f\"üìà Best Coverage: Œµ = {best_coverage_epsilon} (coverage = {best_coverage:.1%})\")\n",
    "    \n",
    "    # Runtime vs accuracy trade-off\n",
    "    runtime_error_corr = df['rps_time'].corr(df['mean_pool_error'])\n",
    "    runtime_coverage_corr = df['rps_time'].corr(df['coverage_ratio'])\n",
    "    print(f\"‚öñÔ∏è  Runtime-Error Correlation: {runtime_error_corr:+.3f}\")\n",
    "    print(f\"‚öñÔ∏è  Runtime-Coverage Correlation: {runtime_coverage_corr:+.3f}\")\n",
    "    \n",
    "    print(\"\\n=== RECOMMENDATIONS ===\")\n",
    "    print(\"üí° For fastest runtime: Use smaller M and R values\")\n",
    "    print(\"üí° For best accuracy: Consider epsilon values around\", best_epsilon)\n",
    "    print(\"üí° For best coverage: Consider epsilon values around\", best_coverage_epsilon)\n",
    "    \n",
    "    if abs(runtime_error_corr) > 0.3:\n",
    "        print(\"‚ö†Ô∏è  Strong runtime-error correlation detected - consider trade-off analysis\")\n",
    "    if abs(runtime_coverage_corr) > 0.3:\n",
    "        print(\"‚ö†Ô∏è  Strong runtime-coverage correlation detected - consider trade-off analysis\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
