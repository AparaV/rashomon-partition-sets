{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61fee253-9c7d-442a-91f4-91ea16747ccc",
   "metadata": {},
   "source": [
    "# Tutorial for the core API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c4957e7-22cc-48a4-bddc-002b47d7a21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66c612e-e270-4972-b1bf-4e69a29f6ead",
   "metadata": {},
   "source": [
    "## Setup parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed424023-050a-410b-ab5c-e27873c1aa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rashomon import hasse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72baab3-7a61-4c6b-bbdf-426f0eb6cd1b",
   "metadata": {},
   "source": [
    "There are 3 features\n",
    "- Feature 1 takes on four values, {0, 1, 2, 3}\n",
    "- Feature 2 takes on three values, {0, 1, 2}\n",
    "- Feature 3 takes on three values, {0, 1, 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1dd38e7-b89e-40fa-a60a-d8bbeef60ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 3\n",
    "R = np.array([4, 3, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8df1377-3160-4c50-847f-1395771c2449",
   "metadata": {},
   "source": [
    "First, we find all the profiles corresponding to this setup. For the profiles, only the number of features matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd169aff-56fe-48af-9ef8-7f2ced8c8d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiles\n",
      "[(0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1), (1, 0, 0), (1, 0, 1), (1, 1, 0), (1, 1, 1)]\n",
      "\n",
      "Map from each profile tuple to its index in `profiles` list\n",
      "{(0, 0, 0): 0, (0, 0, 1): 1, (0, 1, 0): 2, (0, 1, 1): 3, (1, 0, 0): 4, (1, 0, 1): 5, (1, 1, 0): 6, (1, 1, 1): 7}\n"
     ]
    }
   ],
   "source": [
    "num_profiles = 2**M\n",
    "profiles, profile_map = hasse.enumerate_profiles(M)\n",
    "\n",
    "print(\"Profiles\")\n",
    "print(profiles)\n",
    "\n",
    "print(\"\\nMap from each profile tuple to its index in `profiles` list\")\n",
    "print(profile_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46995908-e9de-4bd9-9d5c-995d437306ab",
   "metadata": {},
   "source": [
    "Next, we find all the possible feature combinations (i.e., policies) in our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07d9164d-6061-4905-9b82-c1e69305a33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 36 policies\n",
      "[(0, 0, 0), (0, 0, 1), (0, 0, 2), (0, 1, 0), (0, 1, 1), (0, 1, 2), (0, 2, 0), (0, 2, 1), (0, 2, 2), (1, 0, 0), (1, 0, 1), (1, 0, 2), (1, 1, 0), (1, 1, 1), (1, 1, 2), (1, 2, 0), (1, 2, 1), (1, 2, 2), (2, 0, 0), (2, 0, 1), (2, 0, 2), (2, 1, 0), (2, 1, 1), (2, 1, 2), (2, 2, 0), (2, 2, 1), (2, 2, 2), (3, 0, 0), (3, 0, 1), (3, 0, 2), (3, 1, 0), (3, 1, 1), (3, 1, 2), (3, 2, 0), (3, 2, 1), (3, 2, 2)]\n"
     ]
    }
   ],
   "source": [
    "all_policies = hasse.enumerate_policies(M, R)\n",
    "num_policies = len(all_policies)\n",
    "\n",
    "print(f\"All {num_policies} policies\")\n",
    "print(all_policies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831ae4d9-717c-4bd7-b91f-30e421410496",
   "metadata": {},
   "source": [
    "## Partition matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12fe8be2-a0b0-47dc-a19a-d4bbe90f9e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rashomon import extract_pools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52cfa17-b660-4bc2-af4e-8f664aa063a1",
   "metadata": {},
   "source": [
    "We will look only at the (1, 1, 1) profile for the purpose of illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a5a3c27-079a-4085-bf18-86df2f499e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1, 1),\n",
       " (1, 1, 2),\n",
       " (1, 2, 1),\n",
       " (1, 2, 2),\n",
       " (2, 1, 1),\n",
       " (2, 1, 2),\n",
       " (2, 2, 1),\n",
       " (2, 2, 2),\n",
       " (3, 1, 1),\n",
       " (3, 1, 2),\n",
       " (3, 2, 1),\n",
       " (3, 2, 2)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policies_111 = [x for x in all_policies if x[0] > 0 and x[1] > 0 and x[2] > 0]\n",
    "policies_111"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bd74c3-108d-4b7f-9d85-aebb55a0b9da",
   "metadata": {},
   "source": [
    "Let us say that the partition is as follows:\n",
    "- $\\pi_1$ = {(1, 1, 1), (1, 2, 1)}\n",
    "- $\\pi_2$ = {(1, 1, 2), (1, 2, 2)}\n",
    "- $\\pi_3$ = {(2, 1, 1), (2, 2, 1), (3, 1, 1), (3, 2, 1)}\n",
    "- $\\pi_4$ = {(2, 1, 2), (2, 2, 2), (3, 1, 2), (3, 2, 2)}\n",
    "\n",
    "This corresponds to the following $\\Sigma$ matrix. The `np.inf` implies that that feature does not take those factor levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43860f09-3aa8-4c8e-9e4d-46590ec4e4fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.],\n",
       "       [ 1., inf],\n",
       "       [ 0., inf]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma_111 = np.array([[0, 1],\n",
    "                  [1, np.inf],\n",
    "                  [0, np.inf]])\n",
    "sigma_111"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f239fa0e-32c7-4cc7-bad5-886dd7e736c0",
   "metadata": {},
   "source": [
    "This is how we extract the pools from the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b114782-f1a9-479e-b578-e2466e073d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_pools, pi_policies = extract_pools.extract_pools(policies_111, sigma_111)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566adeff-eb4f-4350-9af2-af0d35e3d2fe",
   "metadata": {},
   "source": [
    "`pi_pools` is a dictionary that maps each pool index to a list of _indices_ of feature combinations in that pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24e3d512-1085-4d51-bbcd-975d04fc02de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [0, 2], 1: [1, 3], 2: [4, 6, 8, 10], 3: [5, 7, 9, 11]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_pools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794b5a70-362c-43d5-9dec-8e893cab633e",
   "metadata": {},
   "source": [
    "`pi_policies` is a dictionary that maps each feature combination (through its index) to the index of the pool it belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1b37949-6984-4c0f-96c2-b0cfcfd1aaec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0, 2: 0, 1: 1, 3: 1, 4: 2, 6: 2, 8: 2, 10: 2, 5: 3, 7: 3, 9: 3, 11: 3}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4953447d-12b7-44b1-b129-2f868560cc7a",
   "metadata": {},
   "source": [
    "`extract_pools` also has an optional argument `lattice_edges` where you provide the edges in the Hasse. If you call `extract_pools` on the same Hasse very often, it is more efficient to pre-compute the lattice edges once and pass in this argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9972309a-cfd8-4a39-b812-91b2e3abc85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hasse_edges = extract_pools.lattice_edges(policies_111)\n",
    "\n",
    "pi_pools, pi_policies = extract_pools.extract_pools(policies_111, sigma_111, lattice_edges=hasse_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5803252e-432b-4bb2-865f-eaa392faa268",
   "metadata": {},
   "source": [
    "## Generate data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c312868-747c-4e60-9bd9-9958a98e87e7",
   "metadata": {},
   "source": [
    "Since there are 4 pools, we only need to select 4 distributions for the outcome. For simplicity, say the outcomes come from $N(\\mu_{\\pi}, \\sigma_{\\pi}^2)$ with the following parameters for each pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3941e43e-f8f6-47a5-a8e6-fe8419daf40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_111 = np.array([0, 2, 4, -2])\n",
    "var_111 = np.array([1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dfa68c-f49d-4ef5-a223-0b30cd90d1dd",
   "metadata": {},
   "source": [
    "Fix 50 samples per feature and generate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1beed23-81e7-4bc8-94ac-9063e2f2bc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "\n",
    "num_samples_per_feature = 50\n",
    "num_data = len(policies_111) * num_samples_per_feature\n",
    "\n",
    "X = np.zeros(shape=(num_data, M))\n",
    "D = np.zeros(shape=(num_data, 1), dtype='int_')\n",
    "y = np.zeros(shape=(num_data, 1))\n",
    "\n",
    "idx_ctr = 0\n",
    "for k, feature in enumerate(policies_111):\n",
    "    # policy_idx = [i for i, x in enumerate(all_policies) if x == policy]\n",
    "\n",
    "    pool_id = pi_policies[k]\n",
    "    mu_i = mu_111[pool_id]\n",
    "    var_i = var_111[pool_id]\n",
    "    y_i = np.random.normal(mu_i, var_i, size=(num_samples_per_feature, 1))\n",
    "\n",
    "    start_idx = idx_ctr * num_samples_per_feature\n",
    "    end_idx = (idx_ctr + 1) * num_samples_per_feature\n",
    "\n",
    "    X[start_idx:end_idx, ] = feature\n",
    "    D[start_idx:end_idx, ] = k\n",
    "    y[start_idx:end_idx, ] = y_i\n",
    "\n",
    "    idx_ctr += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80621973-626a-4b14-944a-97e15178cccf",
   "metadata": {},
   "source": [
    "`X` is the feature matrix.\n",
    "\n",
    "`D` tells us the feature indices i.e., `D[i, 0]` is the feature index of `X[i, ]`.\n",
    "\n",
    "`y` is the outcome vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4eb2400-5658-47e0-b5c3-4a78168a548b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "[[ 1.78862847]\n",
      " [ 0.43650985]\n",
      " [ 0.09649747]\n",
      " [-1.8634927 ]\n",
      " [-0.2773882 ]\n",
      " [-0.35475898]\n",
      " [-0.08274148]\n",
      " [-0.62700068]\n",
      " [-0.04381817]\n",
      " [-0.47721803]]\n"
     ]
    }
   ],
   "source": [
    "print(X[:10,])\n",
    "\n",
    "print(D[:10])\n",
    "\n",
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6af733-4f9a-48e3-975b-f25b6bd6eefc",
   "metadata": {},
   "source": [
    "We can calculate the mean outcome of each feature through the following object. The first column contains the sums of outcomes for each feature. The second column is the count. So dividing the first column by the second lets us find the average. We keep the sums and counts separately for internal computation purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e67cf696-e873-4b93-94b3-10e925b416b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -14.696341     50.        ]\n",
      " [ 103.83263356   50.        ]\n",
      " [   1.45837656   50.        ]\n",
      " [  95.24720387   50.        ]\n",
      " [ 221.50472005   50.        ]\n",
      " [ -92.80830557   50.        ]\n",
      " [ 189.57330566   50.        ]\n",
      " [-107.31751574   50.        ]\n",
      " [ 209.98734846   50.        ]\n",
      " [-103.75982477   50.        ]\n",
      " [ 200.56828936   50.        ]\n",
      " [ -99.03931665   50.        ]]\n"
     ]
    }
   ],
   "source": [
    "from rashomon import loss\n",
    "\n",
    "policy_means_111 = loss.compute_policy_means(D, y, len(policies_111))\n",
    "print(policy_means_111)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12678a17-d4fe-4fe6-9772-b2d0bd9bcecb",
   "metadata": {},
   "source": [
    "## Finding the Rashomon set for profile (1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba6f1fcf-76e2-469c-bd22-57302ca3ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rashomon import aggregate\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a15cd0b-9a0e-44c6-acf5-dd0c37e37b6f",
   "metadata": {},
   "source": [
    "Let us set the maximum number of pools to be $H = \\infty$ and the Rashomon threshold to be $\\theta = 8$ and regularization $\\lambda = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f8eafb3-5703-4c0c-8ddb-dda310488c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = np.inf\n",
    "theta = 8\n",
    "lamb = 1\n",
    "\n",
    "RPS_111 = aggregate.RAggregate_profile(M, R, H, D, y, theta, profile=(1, 1, 1), reg=lamb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f98167a-9a14-4cff-99f2-12e5755d25f3",
   "metadata": {},
   "source": [
    "The output of `RAggregate_profile` is an object of type `RashomonSet`. Here are some useful things we can do with this. Observe that the true partition `sigma_111` is the second partition in the RPS and has the least loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70ff38b7-a2e1-48a7-90be-a2dd5067aa8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.]\n",
      " [ 1. inf]\n",
      " [ 0. inf]]\n",
      "[[ 0.  1.]\n",
      " [ 1. inf]\n",
      " [ 0. inf]]\n",
      "[[ 1.  0.]\n",
      " [ 1. inf]\n",
      " [ 0. inf]]\n",
      "[[ 0.  0.]\n",
      " [ 1. inf]\n",
      " [ 0. inf]]\n",
      "[2. 4. 4. 6.]\n"
     ]
    }
   ],
   "source": [
    "# Show the partition matrix for each member of the RPS\n",
    "for sig in RPS_111.sigma:\n",
    "    print(sig)\n",
    "\n",
    "# Count the number of pools in each Rashomon partition\n",
    "print(RPS_111.pools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dde88f6f-550e-4730-b162-f5023fdd7c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.8207948  5.04016569 7.86851401 7.04010387]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the loss\n",
    "# This needs to be done only when calling `RAggregate_profile`\n",
    "# When calling the main function `RAggregate`, loss is automatically calculated\n",
    "RPS_111.calculate_loss(D, y, policies_111, policy_means_111, reg=lamb)\n",
    "\n",
    "# Print the loss for each member in the RPS\n",
    "print(RPS_111.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3751c5-abe8-417c-98b2-958f5d5125fb",
   "metadata": {},
   "source": [
    "Additionally, there is an internal function that manually checks every single partition to see if it belongs to the RPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7fe09334-1864-44dc-80a6-7b61a99438f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RPS_111_brute_force = aggregate._brute_RAggregate_profile(M, R, H, D, y, theta, profile=(1, 1, 1), reg=lamb)\n",
    "\n",
    "# Verify that the brute force computation matches the branch-and-bound algorithm\n",
    "RPS_111_brute_force.P_hash == RPS_111.P_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53d02fe-1b6a-4d75-bc9e-825dd0494bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20991547-bbc4-47d0-9ed2-05d9a76fd2f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d050566-a84b-426b-8f5c-7b133fe775cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d41234-b46f-4b33-bb11-1cbce41622db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66e93b52-e679-491d-91bd-535cf696a7ad",
   "metadata": {},
   "source": [
    "## For all profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3647be5-7743-4144-88f9-8f47c425aa42",
   "metadata": {},
   "source": [
    "Fix the partition matrices and outcome parameters for all other profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08aa0ad7-8f1a-4ac2-90cb-59fe5054d67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile (0, 0, 0)\n",
    "sigma_000 = None\n",
    "mu_000 = np.array([0])\n",
    "var_000 = np.array([1])\n",
    "\n",
    "# Profile (0, 0, 1)\n",
    "sigma_001 = np.array([[1]])\n",
    "mu_001 = np.array([-2])\n",
    "var_001 = np.array([1])\n",
    "\n",
    "# Profile (0, 1, 0)\n",
    "sigma_010 = np.array([[1]])\n",
    "mu_010 = np.array([1])\n",
    "var_010 = np.array([1])\n",
    "\n",
    "# Profile (0, 1, 1)\n",
    "sigma_011 = np.array([[1], [0]])\n",
    "mu_011 = np.array([1, -2])\n",
    "var_011 = np.array([1, 1])\n",
    "\n",
    "# Profile (1, 0, 0)\n",
    "sigma_100 = np.array([[0, 1]])\n",
    "mu_100 = np.array([0, 2])\n",
    "var_100 = np.array([1, 1])\n",
    "\n",
    "# Profile (1, 0, 1)\n",
    "sigma_101 = np.array([[0, 1], [0, np.inf]])\n",
    "mu_101 = np.array([0, 2, 1, -2])\n",
    "var_101 = np.array([1, 1, 1, 1])\n",
    "\n",
    "# Profile (1, 1, 0)\n",
    "sigma_110 = np.array([[0, 1], [1, np.inf]])\n",
    "mu_110 = np.array([0, -2])\n",
    "var_110 = np.array([1, 1])\n",
    "\n",
    "sigma = [sigma_000, sigma_001, sigma_010, sigma_011, sigma_100, sigma_101, sigma_110, sigma_111]\n",
    "mu = [mu_000, mu_001, mu_010, mu_011, mu_100, mu_101, mu_110, mu_111]\n",
    "var = [var_000, var_001, var_010, var_011, var_100, var_101, var_110, var_111]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e3d97a-7344-4f8d-80ea-4b33f3a6bfe0",
   "metadata": {},
   "source": [
    "### Find all pools for each profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81b041f-f56d-405a-b020-0eaa3770cffe",
   "metadata": {},
   "source": [
    "This code block does what we did previously for a single profile. Since `extract_pools` only works for indexing within a Hasse, we need to carefully map the universal indexing of features across all profiles to its corresponding index within the profile that it belongs to. This is why this code chunk appears more complicated than it actually is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc7f0144-a6b5-468e-8611-3a50944d97d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "policies_profiles = {}\n",
    "policies_profiles_masked = {}\n",
    "policies_ids_profiles = {}\n",
    "pi_policies = {}\n",
    "pi_pools = {}\n",
    "for k, profile in enumerate(profiles):\n",
    "\n",
    "    policies_temp = [(i, x) for i, x in enumerate(all_policies) if hasse.policy_to_profile(x) == profile]\n",
    "    unzipped_temp = list(zip(*policies_temp))\n",
    "    policies_ids_k = list(unzipped_temp[0])\n",
    "    policies_k = list(unzipped_temp[1])\n",
    "    policies_profiles[k] = deepcopy(policies_k)\n",
    "    policies_ids_profiles[k] = policies_ids_k\n",
    "\n",
    "    profile_mask = list(map(bool, profile))\n",
    "\n",
    "    # Mask the empty arms\n",
    "    for idx, pol in enumerate(policies_k):\n",
    "        policies_k[idx] = tuple([pol[i] for i in range(M) if profile_mask[i]])\n",
    "    policies_profiles_masked[k] = policies_k\n",
    "\n",
    "    if np.sum(profile) > 0:\n",
    "        pi_pools_k, pi_policies_k = extract_pools.extract_pools(policies_k, sigma[k])\n",
    "        if len(pi_pools_k.keys()) != mu[k].shape[0]:\n",
    "            print(f\"Profile {k}. Expected {len(pi_pools_k.keys())} pools. Received {mu[k].shape[0]} means.\")\n",
    "        pi_policies[k] = pi_policies_k\n",
    "        # pi_pools_k has indicies that match with policies_profiles[k]\n",
    "        # Need to map those indices back to all_policies\n",
    "        pi_pools[k] = {}\n",
    "        for x, y in pi_pools_k.items():\n",
    "            y_full = [policies_profiles[k][i] for i in y]\n",
    "            y_agg = [all_policies.index(i) for i in y_full]\n",
    "            pi_pools[k][x] = y_agg\n",
    "    else:\n",
    "        pi_policies[k] = {0: 0}\n",
    "        pi_pools[k] = {0: [0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b22cf3d-1f91-4f14-9c0d-9f0960ff6254",
   "metadata": {},
   "source": [
    "### Generate data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb220e32-9eed-4c80-a7c3-4e118de60289",
   "metadata": {},
   "source": [
    "Again, this repeats what we did for a single profile for all profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a4f40da-b998-4feb-b474-43c6a6507eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(mu, var, n_per_pol, all_policies, pi_policies, M):\n",
    "    num_data = num_policies * n_per_pol\n",
    "    X = np.zeros(shape=(num_data, M))\n",
    "    D = np.zeros(shape=(num_data, 1), dtype='int_')\n",
    "    y = np.zeros(shape=(num_data, 1))\n",
    "\n",
    "    idx_ctr = 0\n",
    "    for k, profile in enumerate(profiles):\n",
    "        policies_k = policies_profiles[k]\n",
    "\n",
    "        for idx, policy in enumerate(policies_k):\n",
    "            policy_idx = [i for i, x in enumerate(all_policies) if x == policy]\n",
    "\n",
    "            pool_id = pi_policies[k][idx]\n",
    "            mu_i = mu[k][pool_id]\n",
    "            var_i = var[k][pool_id]\n",
    "            y_i = np.random.normal(mu_i, var_i, size=(n_per_pol, 1))\n",
    "\n",
    "            start_idx = idx_ctr * n_per_pol\n",
    "            end_idx = (idx_ctr + 1) * n_per_pol\n",
    "\n",
    "            X[start_idx:end_idx, ] = policy\n",
    "            D[start_idx:end_idx, ] = policy_idx[0]\n",
    "            y[start_idx:end_idx, ] = y_i\n",
    "\n",
    "            idx_ctr += 1\n",
    "\n",
    "    return X, D, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27329883-5733-4ba3-92fa-61fbc62b3e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_per_feature = 50000\n",
    "\n",
    "X, D, y = generate_data(mu, var, num_samples_per_feature, all_policies, pi_policies, M)\n",
    "policy_means = loss.compute_policy_means(D, y, num_policies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799e1b79-d311-4c61-a9ea-45c87a0be15d",
   "metadata": {},
   "source": [
    "### Finding the Rashomon Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27182704-9af8-4888-922e-13f927e22313",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = np.inf\n",
    "theta = 13\n",
    "lamb = 1\n",
    "\n",
    "R_set, R_profiles = aggregate.RAggregate(M, R, H, D, y, theta, reg=lamb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e1f2ea-e56a-4b80-bbb4-bbdd077be43f",
   "metadata": {},
   "source": [
    "The output of `RAggregate` is different from that of `RAggregate_profile`. For starters, the output is a tuple.\n",
    "\n",
    "The first item `R_set` is a list. Each item in `R_set` is a list itself. The length of this list is the number of profiles. Each item in `R_set[i]` gives an index for a partition of that profile. So `R_set[i][k]` is the partition of the k-th profile in the i-th Rashomon partition in the RPS.\n",
    "\n",
    "The second item `R_profiles` is a list whose length is the number of profiles. Each item is the `RashomonSet` object that we saw earlier. The indices in `R_set` correspond to the partitions in `R_profiles`. So the actual partition of `R_set[i][k]` is retrieved by accessing `R_profiles[k].sigma[R_set[i][k]]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "727f5eb3-e89f-4bfc-8604-2fabd479c0a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 0, 0, 0, 2],\n",
       " [0, 0, 0, 0, 0, 0, 0, 3],\n",
       " [0, 0, 0, 0, 0, 0, 0, 4],\n",
       " [0, 0, 0, 0, 0, 0, 0, 5],\n",
       " [0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1, 1],\n",
       " [0, 0, 0, 0, 0, 0, 2, 0],\n",
       " [0, 0, 0, 0, 0, 0, 3, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0, 1],\n",
       " [0, 0, 0, 0, 0, 2, 0, 0],\n",
       " [0, 0, 0, 0, 0, 2, 0, 1],\n",
       " [0, 0, 0, 0, 0, 3, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 1],\n",
       " [0, 0, 0, 0, 2, 0, 0, 0],\n",
       " [0, 0, 0, 1, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 0, 0, 0, 1],\n",
       " [0, 0, 0, 2, 0, 0, 0, 0],\n",
       " [0, 0, 1, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e6cbb53-c2c4-45af-ac6b-675e20f2aca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[None],\n",
       " [array([[1.]]), array([[0.]])],\n",
       " [array([[1.]]), array([[0.]])],\n",
       " [array([[1.],\n",
       "        [1.]]), array([[1.],\n",
       "        [0.]]), array([[0.],\n",
       "        [1.]])],\n",
       " [array([[1., 1.]]), array([[0., 1.]]), array([[1., 0.]])],\n",
       " [array([[ 1.,  1.],\n",
       "        [ 1., inf]]), array([[ 0.,  1.],\n",
       "        [ 1., inf]]), array([[ 1.,  1.],\n",
       "        [ 0., inf]]), array([[ 1.,  0.],\n",
       "        [ 1., inf]])],\n",
       " [array([[ 1.,  1.],\n",
       "        [ 1., inf]]), array([[ 0.,  1.],\n",
       "        [ 1., inf]]), array([[ 1.,  0.],\n",
       "        [ 1., inf]]), array([[ 1.,  1.],\n",
       "        [ 0., inf]])],\n",
       " [array([[ 1.,  1.],\n",
       "        [ 1., inf],\n",
       "        [ 1., inf]]), array([[ 1.,  1.],\n",
       "        [ 1., inf],\n",
       "        [ 0., inf]]), array([[ 0.,  1.],\n",
       "        [ 1., inf],\n",
       "        [ 0., inf]]), array([[ 0.,  1.],\n",
       "        [ 1., inf],\n",
       "        [ 1., inf]]), array([[ 1.,  0.],\n",
       "        [ 1., inf],\n",
       "        [ 1., inf]]), array([[ 1.,  1.],\n",
       "        [ 0., inf],\n",
       "        [ 1., inf]])]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a881e443-aeb3-4ccb-83c5-e7a7f893efab",
   "metadata": {},
   "source": [
    "Now, let us see how to access these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b629a28-fd0b-47e9-8608-64b64797382f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile (0, 0, 0)\n",
      "Partition\n",
      "None\n",
      "Loss = 1.0276327713315392\n",
      "Number of pools = 1\n",
      "---\n",
      "Profile (0, 0, 1)\n",
      "Partition\n",
      "[[1.]]\n",
      "Loss = 1.0549152190530173\n",
      "Number of pools = 1.0\n",
      "---\n",
      "Profile (0, 1, 0)\n",
      "Partition\n",
      "[[1.]]\n",
      "Loss = 1.0554461937366306\n",
      "Number of pools = 1.0\n",
      "---\n",
      "Profile (0, 1, 1)\n",
      "Partition\n",
      "[[1.]\n",
      " [1.]]\n",
      "Loss = 1.3612916033997204\n",
      "Number of pools = 1.0\n",
      "---\n",
      "Profile (1, 0, 0)\n",
      "Partition\n",
      "[[1. 1.]]\n",
      "Loss = 1.157395257884124\n",
      "Number of pools = 1.0\n",
      "---\n",
      "Profile (1, 0, 1)\n",
      "Partition\n",
      "[[ 1.  1.]\n",
      " [ 1. inf]]\n",
      "Loss = 1.5540328652364819\n",
      "Number of pools = 1.0\n",
      "---\n",
      "Profile (1, 1, 0)\n",
      "Partition\n",
      "[[ 1.  1.]\n",
      " [ 1. inf]]\n",
      "Loss = 1.3142552442601412\n",
      "Number of pools = 1.0\n",
      "---\n",
      "Profile (1, 1, 1)\n",
      "Partition\n",
      "[[ 0.  1.]\n",
      " [ 1. inf]\n",
      " [ 1. inf]]\n",
      "Loss = 4.444784097194952\n",
      "Number of pools = 2.0\n",
      "---\n",
      "Total loss = 12.969753252096606, Total number of pools = 9.0\n"
     ]
    }
   ],
   "source": [
    "i = 3\n",
    "\n",
    "RPS_partitions_i = R_set[i]\n",
    "\n",
    "total_loss = 0\n",
    "total_pools = 0\n",
    "for k, profile in enumerate(profiles):\n",
    "    print(\"Profile\", profile)\n",
    "\n",
    "    R_partition_i_k = R_profiles[k].sigma[RPS_partitions_i[k]]\n",
    "    print(\"Partition\")\n",
    "    print(R_partition_i_k)\n",
    "\n",
    "    # Notice that unlike the per-profile case, the loss of this partition is already pre-computed\n",
    "    loss_i_k = R_profiles[k].loss[RPS_partitions_i[k]]\n",
    "    print(f\"Loss = {loss_i_k}\")\n",
    "    \n",
    "    pools_i_k = R_profiles[k].pools[RPS_partitions_i[k]]\n",
    "    print(f\"Number of pools = {pools_i_k}\")\n",
    "\n",
    "    total_loss += loss_i_k\n",
    "    total_pools += pools_i_k\n",
    "\n",
    "    print(\"---\")\n",
    "\n",
    "print(f\"Total loss = {total_loss}, Total number of pools = {total_pools}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccfcc17-f36c-426c-b9c8-69cf3484a4f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf8fa212-9c7f-4f70-bf00-4c6d9e9798e8",
   "metadata": {},
   "source": [
    "By default `RAggregate` uses only one process. But we can parallelize finding Rashomon sets for each profile by changing the `num_workers` argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "725baf19-c7ce-442f-af3d-04f3373b6bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f7ed10c4-5b0a-4a25-be00-429e222745d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 1 worker, RAggregate took 9.232525825500488 s.\n"
     ]
    }
   ],
   "source": [
    "# num_workers = 1\n",
    "\n",
    "start = time.time()\n",
    "R_set1, R_profiles1 = aggregate.RAggregate(M, R, H, D, y, theta, reg=lamb, num_workers=1)\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "\n",
    "print(f\"With 1 worker, RAggregate took {elapsed} s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f3f67ebd-c1dd-4069-8343-e994d547b949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 2 workers, RAggregate took 8.007990837097168 s.\n"
     ]
    }
   ],
   "source": [
    "# num_workers = 2\n",
    "\n",
    "start = time.time()\n",
    "R_set2, R_profiles2 = aggregate.RAggregate(M, R, H, D, y, theta, reg=lamb, num_workers=2)\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "\n",
    "print(f\"With 2 workers, RAggregate took {elapsed} s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84d8bb53-4824-4f38-8bb0-c790a50606aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Check whether the results are the same\n",
    "print(R_set1 == R_set2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e037ab-3cdd-4f91-87f5-425fd8758b7e",
   "metadata": {},
   "source": [
    "The difference of 1 second seems negligible but the gains will be more substantial when there are more features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aa7e1e-90a8-47a6-a762-65b3428221a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rashomon-tva)",
   "language": "python",
   "name": "rashomon-tva"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
